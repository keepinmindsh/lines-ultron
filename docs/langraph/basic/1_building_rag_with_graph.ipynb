{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab83ddf",
   "metadata": {},
   "source": [
    "# Naive RAG to LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97798d8",
   "metadata": {},
   "source": [
    "### 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc701649",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# update or install the necessary libraries\n",
    "\n",
    "%pip install langgraph\n",
    "%pip install --upgrade \\\n",
    "    langchain==0.3.24 \\\n",
    "    langchain-openai==0.3.14 \\\n",
    "    langchain_community\n",
    "%pip install -U langsmith\n",
    "%pip install --upgrade python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e70668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8660e0",
   "metadata": {},
   "source": [
    "### LangSmith 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "539d47a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How may I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "@traceable  # LangSmith 추적용 데코레이터\n",
    "def pipeline(user_input: str):\n",
    "    result = llm.invoke([HumanMessage(content=user_input)])\n",
    "    return result.content  # or result if full object needed\n",
    "\n",
    "print(pipeline(\"Hello, world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a2c63f",
   "metadata": {},
   "source": [
    "### 기본 PDF 기반 Retrieval Chain 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2950a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import ElasticsearchStore\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os \n",
    "\n",
    "\n",
    "class DataPipeline:\n",
    "  def __init__(self):\n",
    "    self.file_path = \"/Users/lines/sources/00_company/lines-rag-processor-python/load/use_case/medical_study_books_0_10.pdf\"\n",
    "    self.data_loader = PyPDFLoader(self.file_path)\n",
    "    self.embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", dimensions=1024)\n",
    "    self.elastic_store = ElasticsearchStore(\n",
    "        embedding=self.embeddings,\n",
    "        es_url=os.getenv(\"ES_URL\"),\n",
    "        es_user=os.getenv(\"ES_USER\"),\n",
    "        es_password=os.getenv(\"ES_PASSWORD\"),\n",
    "        index_name=os.getenv(\"INDEX_NAME\")\n",
    "    )\n",
    "    \n",
    "\n",
    "  def load(self) -> List[Document]:\n",
    "    pages = []\n",
    "    for page in self.data_loader.load():\n",
    "      pages.append(page)\n",
    "\n",
    "  \n",
    "  def splits(self, documents:List[Document]) -> List[Document]:\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\\n\",\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "  \n",
    "    return docs\n",
    "  \n",
    "  def store(self, documents:List[Document]):\n",
    "    instance_from_documents = self.elastic_store.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=self.embeddings,\n",
    "        index_name=os.getenv(\"INDEX_NAME\"),\n",
    "        es_url=os.getenv(\"ES_URL\"),\n",
    "    )\n",
    "\n",
    "    instance_from_documents.client.indices.refresh(index=os.getenv(\"INDEX_NAME\"))\n",
    "\n",
    "\n",
    "  def retriever(self) -> VectorStoreRetriever:\n",
    "        ## Setup the Retriever using an In-memory store\n",
    "    ## , which can retrieve documents based on a query\n",
    "    retriever = self.elastic_store.as_retriever()\n",
    "\n",
    "    return retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848c3aa0",
   "metadata": {},
   "source": [
    "### Langgraph로 구현 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d9bed",
   "metadata": {},
   "source": [
    "#### State 정의 \n",
    "\n",
    "- State : Graph의 노드와 노드 간 공유하는 상태를 정의합니다 \n",
    "- 일반적으로 TypedDict 형식을 사용합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "993388e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "  question: Annotated[str, \"Question\"]\n",
    "  context: Annotated[str, \"Context\"]\n",
    "  answer: Annotated[str, \"Answer\"]\n",
    "  messages: Annotated[list, add_messages]\n",
    "  relevances: Annotated[str, \"Relervance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e435d2",
   "metadata": {},
   "source": [
    "#### Node 정의 \n",
    "\n",
    "- Nodes: 각 단계를 처리하는 노드입니다. 보통은 Python 함수로 구현합니다. 입력곽 출력이 상태(State) 값입니다. \n",
    "- State를 입력으로 받아 정의된 로직을 수행한 후 업데이트된 State를 반환합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0f3c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_document(state: GraphState) -> GraphState:\n",
    "  latest_question = state[\"question\"]\n",
    "\n",
    "  # retreive from vector db for rag\n",
    "\n",
    "  # format docs\n",
    "\n",
    "  return { \"context\" : \"\" }\n",
    "\n",
    "def llm_answer(state: GraphState) -> GraphState:\n",
    "  latest_question = state[\"question\"]\n",
    "\n",
    "  context = state[\"context\"]\n",
    "\n",
    "  # history handling \n",
    "\n",
    "  # answer handling\n",
    "\n",
    "  return {\n",
    "    \"answer\" : \"my answer\", \n",
    "    \"messages\" : [(\"user\", latest_question), (\"assistant\", \"response\")]\n",
    "  }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b9cab5",
   "metadata": {},
   "source": [
    "#### 그래프 생성 \n",
    "\n",
    "- Edges : 현재 State를 기반으로 다음에 실행할 Node를 결정하는 Python 함수 \n",
    "  - 일반 엣지\n",
    "  - 조건부 엣지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed0f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from IPython.display import Image, display\n",
    "\n",
    "graph = StateGraph(GraphState)\n",
    "graph.add_node(\"retrieve\", retrieve_document)\n",
    "graph.add_node(\"llm_answer\", llm_answer)\n",
    "\n",
    "graph.add_edge(\"retrieve\", \"llm_answer\")\n",
    "graph.add_edge(\"llm_answer\", END)\n",
    "\n",
    "graph.set_entry_point(\"retrieve\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "app = graph.compile(checkpointer=memory)\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png(\n",
    "        max_retries=5,\n",
    "        retry_delay=2.0\n",
    "    )))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c4d0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\" : [ (\"human\", \"AI 관련된 최신 뉴스를 검색해줘\")]}\n",
    "\n",
    "async for chunk_msg, metadata in graph.astream(inputs, stream_mode=\"messages\"):\n",
    "  # chatbot 노드에서 출력된 메시지만 출력\n",
    "  if metadata[\"langgraph_node\"] == \"chatbot\" :\n",
    "    if chunk_msg.content:\n",
    "      print(chunk_msg.content, end=\"\", flush=True)\n",
    "  else:\n",
    "    print(chunk_msg.content)\n",
    "  \n",
    "\n",
    "outputs = app.get_state(config).values \n",
    "\n",
    "print(f'Question: {outputs[\"question\"]}')\n",
    "print(\"===\" * 20)\n",
    "print(f'Anwser: {outputs[\"answer\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68745d0a",
   "metadata": {},
   "source": [
    "# Langgraph Tips\n",
    "\n",
    "- Node/Edges 설계를 사전에 그려볼 것! \n",
    "- Node 에서 State 반환은 Dictionary 또는 실제 해당 State 객체를 반환하는 식으로 하면 됩니다. \n",
    "- 멀티 턴의 대화에 대해서 Messages에 Reducers를 이용해서 add_message 를 이용해서 처리할 수 있음 \n",
    "  - 대화이력을 어떤 방식으로 맥락 검색에 사용할지는 업무 도메인에 따라서 달라질 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96219802",
   "metadata": {},
   "source": [
    "# 관련성 체크 ( Relervance Check )\n",
    "\n",
    "- 검색된 문서에 대해 관련성을 체크하는 기능의 추가 \n",
    "  - LLM as Judge 를 활용해보기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0453f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundednessChecker:\n",
    "  def __init__(self, llm, target):\n",
    "    self.llm = llm \n",
    "    self.target = target\n",
    "  \n",
    "  def create():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3409eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_check(state: GraphState) -> GraphState:\n",
    "  question_retrieval_relevant = GroundednessChecker(\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0), target=\"question-retrieval\"\n",
    "  ).create()\n",
    "\n",
    "  response = question_retrieval_relevant.invoke(\n",
    "    {\"question\": state[\"question\"], \"context\": state['context']}\n",
    "  )\n",
    "\n",
    "  print(\"==== [RELEVANCE CHECK] ====\")\n",
    "  print(response.score)\n",
    "\n",
    "  return {\"relevance\" : response.score}\n",
    "\n",
    "def is_relevant(state: GraphState) -> GraphState:\n",
    "  if state[\"relevance\"] == \"yes\":\n",
    "    return \"relevant\"\n",
    "  else: \n",
    "    return \"not relevant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a7d454",
   "metadata": {},
   "source": [
    "> [Groundness Checker 구성 방식](https://github.com/teddylee777/langchain-teddynote/blob/main/langchain_teddynote/evaluator.py)\n",
    "\n",
    "Groundness Checker를 수행한 후 실제 값이 없을 경우에는 Retrieve 시점에 Query Rewrite를 해서 다시 질문을 할 수 있는 구조로 작성되어야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31857820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from IPython.display import Image, display\n",
    "\n",
    "graph = StateGraph(GraphState)\n",
    "graph.add_node(\"retrieve\", retrieve_document)\n",
    "graph.add_node(\"llm_answer\", llm_answer)\n",
    "graph.add_node(\"relevant_check\", relevance_check)\n",
    "\n",
    "graph.add_edge(\"retrieve\", \"relevant_check\")\n",
    "graph.add_conditional_edges(\n",
    "    \"relevant_check\",\n",
    "    is_relevant, \n",
    "    {\n",
    "        \"relevant\" : \"llm_answer\", \n",
    "        \"not relevant\": \"retrieve\"\n",
    "    }\n",
    ")\n",
    "graph.add_edge(\"llm_answer\", END)\n",
    "\n",
    "graph.set_entry_point(\"retrieve\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "app = graph.compile(checkpointer=memory)\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
