{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# update or install the necessary libraries\n",
    "%pip install --upgrade openai\n",
    "%pip install --upgrade langchain\n",
    "%pip install --upgrade langchain_community\n",
    "%pip install --upgrade python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import IPython\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# API configuration\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# for LangChain\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_open_params(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=256,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "):\n",
    "    \"\"\" set openai parameters\"\"\"\n",
    "\n",
    "    openai_params = {}    \n",
    "\n",
    "    openai_params['model'] = model\n",
    "    openai_params['temperature'] = temperature\n",
    "    openai_params['max_tokens'] = max_tokens\n",
    "    openai_params['top_p'] = top_p\n",
    "    openai_params['frequency_penalty'] = frequency_penalty\n",
    "    openai_params['presence_penalty'] = presence_penalty\n",
    "    return openai_params\n",
    "\n",
    "def get_completion(params, messages):\n",
    "    \"\"\" GET completion from openai api\"\"\"\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model = params['model'],\n",
    "        messages = messages,\n",
    "        temperature = params['temperature'],\n",
    "        max_tokens = params['max_tokens'],\n",
    "        top_p = params['top_p'],\n",
    "        frequency_penalty = params['frequency_penalty'],\n",
    "        presence_penalty = params['presence_penalty'],\n",
    "    )\n",
    "    return response\n",
    "\n",
    "params = set_open_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Prompt \n",
    "\n",
    "추가적인 학습 없이 새로운 데이터에 대한 예측을 할 수 있게 하는 기법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "빨간색 자동차는 일반적으로 자동차의 색상과 유형을 기준으로 분류할 수 있습니다. 예를 들어:\n",
       "\n",
       "1. **색상**: 빨간색\n",
       "2. **형태/유형**: 세단, SUV, 해치백, 스포츠카 등\n",
       "3. **브랜드**: 현대, 기아, 토요타, 포드 등\n",
       "4. **연료 유형**: 가솔린, 디젤, 전기차 등\n",
       "\n",
       "더 구체적인 분류를 원하시면 추가적인 정보를 제공해 주세요!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"이미지를 분류하세요:빨간색 자동차\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(params, messages)\n",
    "IPython.display.Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"이미지를 분류하세요:빨간색 자동차\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(params, messages)\n",
    "IPython.display.Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Positive"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Classify the text into neutral, negative, or positive.\n",
    "Generate only the class, nothing else.\n",
    "Text: I think the food was awesome.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(params, messages)\n",
    "IPython.display.Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-of-Thought (COT) Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's identify the odd numbers in the group: 15, 5, 13, 7, and 1. \n",
       "\n",
       "Now, let's add them together:\n",
       "- 15 + 5 = 20\n",
       "- 20 + 13 = 33\n",
       "- 33 + 7 = 40\n",
       "- 40 + 1 = 41\n",
       "\n",
       "The sum of the odd numbers (15, 5, 13, 7, 1) is 41, which is odd.\n",
       "\n",
       "Therefore, the statement is False."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(params, messages)\n",
    "IPython.display.Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Prompt\n",
    "\n",
    "모델의 작은 수의 예시를 제공하는 Prompt 기법입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To determine if the sum of the odd numbers in the group (15, 32, 5, 13, 82, 7, 1) is even or odd, we first identify the odd numbers:\n",
       "\n",
       "- Odd numbers: 15, 5, 13, 7, 1\n",
       "\n",
       "Now we add them together:\n",
       "\n",
       "15 + 5 + 13 + 7 + 1 = 41\n",
       "\n",
       "Since 41 is an odd number, the statement \"The odd numbers in this group add up to an even number\" is False.\n",
       "\n",
       "A: The answer is False."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(params, messages)\n",
    "IPython.display.Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#MoviesLover #Movies #MyMovies #BestMovie #MovieOfTheYear"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Genereate a single line of hashtags for the given topic by in the same style as the following examples:\n",
    "\n",
    "Topic: Books\n",
    "#BooksLover #Books #MyBooks #BestBook #BookOfTheYear\n",
    "\n",
    "Topic: Games\n",
    "#GamesLover #Games #MyGames #BestGame #GameOfTheYear\n",
    "\n",
    "Topic: Movies\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(params, messages)\n",
    "IPython.display.Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot - Chain of Thought "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's break this down step by step:\n",
       "\n",
       "1. **Initial Cupcakes Baked:**\n",
       "   - You baked 15 cupcakes.\n",
       "   - **Total cupcakes now: 15**\n",
       "\n",
       "2. **Cupcakes Given to Friends:**\n",
       "   - You gave 5 cupcakes to your friends.\n",
       "   - **Total cupcakes remaining: 15 - 5 = 10**\n",
       "\n",
       "3. **Cupcakes Baked Again:**\n",
       "   - You baked 4 more cupcakes.\n",
       "   - **Total cupcakes now: 10 + 4 = 14**\n",
       "\n",
       "4. **Cupcake Eaten by You:**\n",
       "   - You ate 1 cupcake.\n",
       "   - **Total cupcakes remaining: 14 - 1 = 13**\n",
       "\n",
       "So, after all these steps, you have **13 cupcakes** left."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"I baked 15 cupcakes for a bake sale. I wanted to share some\n",
    "with my friends so I gave 5 to my friends. Needing a few more\n",
    "for taste testing, I baked 4 more cupcakes.\n",
    "Later, I couldn't resist and a I ate 1 cupcake by myself.\n",
    "How many cupcakes do I have right now? Explain your thinking step by step\n",
    "including the number of cupcakes per step.\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(params, messages)\n",
    "IPython.display.Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree of Thoughts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### 토론 주제: 1kg의 깃털과 1kg의 돌 중 어느 것이 더 무거운가?\n",
       "\n",
       "#### 전문가 1: 김박사\n",
       "안녕하세요, 여러분. 오늘의 주제에 대해 이야기해 보겠습니다. 간단히 말하자면, 1kg의 깃털과 1kg의 돌은 무게가 동일합니다. 두 물체 모두 1kg이기 때문에, 무게를 비교할 필요가 없습니다. 그러나 우리가 이 질문을 던졌을 때, 아마도 우리는 각 물체의 부피나 밀도를 생각할 것입니다.\n",
       "\n",
       "#### 전문가 2: 이교수\n",
       "맞습니다, 김박사님. 하지만 제가 강조하고 싶은 것은, 사람들은 종종 무게와 부피를 혼동합니다. 1kg의 깃털은 많은 공간을 차지하고, 1kg의 돌은 상대적으로 작은 공간을 차지합니다. 그래서 일반적인 사람들은 깃털이 더 가볍다고 느낄 수 있지만, 실제로는 두 물체의 무게는 동일하다는 점을 잊지 말아야 합니다.\n",
       "\n",
       "#### 전문가 3: 박연구원\n",
       "저도 동의합니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Simulate a multi turn debate of three korean experts using korean.\n",
    "They need to answer the following question: Which is heavier? 1kg of feathers or 1kg of stones?\n",
    "They need to debate in rounds and provide explaination until they reach the same conclusion. \"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(params, messages)\n",
    "IPython.display.Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Chaining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To convert the cities into a valid Python list, we will first extract the names, convert them to uppercase, and remove any unnecessary characters. The names from the text are:\n",
       "\n",
       "1. 김박사\n",
       "2. 이교수\n",
       "3. 정선생\n",
       "\n",
       "Now, let's convert them to uppercase and create a Python list:\n",
       "\n",
       "```python\n",
       "cities = [\"김박사\", \"이교수\", \"정선생\"]\n",
       "cities = [city.upper() for city in cities]\n",
       "```\n",
       "\n",
       "After applying the transformations, the final list in uppercase will be:\n",
       "\n",
       "```python\n",
       "cities = [\"김박사\", \"이교수\", \"정선생\"]\n",
       "```\n",
       "\n",
       "Please note that since these are Korean names, they will remain in Hangul but will be represented in uppercase as per your instruction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_1 = \"\"\"\n",
    "Extract all city names from the following text:\n",
    "\n",
    "The aroma of fresh bread wafted through the Paris market, tempting Amelia\n",
    "as she hurried to catch her flight to Tokyo. She dreamt of indulging in steaming\n",
    "ramen after a whirlwind tour of ancient temples. Back home in Chicago,\n",
    "she'd recount her adventures, photos filled with Eiffel Tower selfies\n",
    "and neon-lit Tokyo nights.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_completion(params, messages)\n",
    "IPython.display.Markdown(response.choices[0].message.content)\n",
    "\n",
    "prompt_2 = f\"\"\"Convert the following cities into a valid Python list.\n",
    "Make it uppercase and remove all unecessary characters:\\n{response.choices[0].message.content}\"\"\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt_2\n",
    "    }\n",
    "]\n",
    "\n",
    "response_2 = get_completion(params, messages)\n",
    "IPython.display.Markdown(response_2.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
