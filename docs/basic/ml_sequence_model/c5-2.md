# NLP와 워드 임베딩: 핵심 개념과 응용

## 소개

**자연어 처리(NLP)**는 텍스트와 음성을 이해하고 처리하는 데 중점을 둔 AI의 한 분야입니다.
그 중심에는 **워드 임베딩(Word Embedding)**이 있으며, 이는 단어를 컴퓨터가 처리 가능한 벡터로 변환하여 단어 간 의미적 관계를 학습합니다. 워드 임베딩은 현대 NLP 시스템에서 핵심적인 구성 요소로 사용됩니다.

---

## 목차

1. [워드 표현](#워드-표현)
   - [원-핫 인코딩](#원-핫-인코딩)
   - [워드 임베딩](#워드-임베딩)
2. [워드 임베딩의 활용](#워드-임베딩의-활용)
   - [개체명 인식](#개체명-인식)
   - [전이 학습](#전이-학습)
3. [워드 임베딩의 특성](#워드-임베딩의-특성)
   - [유사도와 관계](#유사도와-관계)
   - [코사인 유사도](#코사인-유사도)
4. [워드 임베딩 학습 방법](#워드-임베딩-학습-방법)
   - [Word2Vec](#word2vec)
   - [GloVe](#glove)
5. [감정 분류](#감정-분류)
6. [워드 임베딩에서의 편향과 제거](#워드-임베딩에서의-편향과-제거)

---

## 워드 표현

### 원-핫 인코딩

- **정의**: 단어를 고유한 이진 벡터로 표현.
  - 예: "apple" → `[1, 0, 0, 0]`, "orange" → `[0, 1, 0, 0]`
- **한계**:
  1. **고차원 문제**: 단어 수가 많아질수록 벡터의 길이가 증가.
  2. **의미 관계 부족**: "king"과 "queen" 간의 관계를 나타낼 수 없음.

### 워드 임베딩

- **정의**: 단어를 저차원 밀집 벡터로 표현하여 단어 간의 의미적 관계를 학습.
  - 예: "king" → `[0.25, 0.56, -0.15, 0.88]`, "queen" → `[0.28, 0.60, -0.20, 0.85]`
- **특징**:
  - 단어 간 유사성 측정 가능.
  - 공간 상에서 유사 단어는 가까이 배치.

#### 시각화

- t-SNE 또는 PCA를 사용해 워드 임베딩을 2D로 표현.
  - 예: "king"과 "queen"이 같은 방향성(성별)을 갖고 있지만 다른 축에서 차이를 보임(왕족).

---

## 워드 임베딩의 활용

### 개체명 인식(NER)

- **정의**: 문장에서 이름, 장소, 조직 등 고유 명사를 인식.
  - 예: `"Sally Johnson is an orange farmer"` → `["Sally Johnson"=Person, "orange"=Food]`
- **활용 사례**:
  - 검색 엔진에서 중요한 키워드 추출.
  - 의료 데이터 분석에서 질병명 및 약물명 인식.

### 전이 학습

- 대규모 데이터셋에서 학습된 임베딩을 소규모 데이터셋으로 전이.
  - 1단계: 대규모 텍스트(수십~수백억 단어)로 워드 임베딩 학습.
  - 2단계: 학습된 임베딩을 소규모 데이터셋(10만 단어)에서 미세 조정(fine-tuning).

---

## 워드 임베딩의 특성

### 유사도와 관계

- 워드 임베딩 벡터는 의미적 관계를 보존.
  - 예: king - man + woman ≈ queen
- **아날로지(유추)**:
  - "Man:Woman as King:Queen"
  - "Ottawa:Canada as Nairobi:Kenya"

### 코사인 유사도

- 두 벡터 간의 방향적 유사성을 측정:
  $$
  \text{Cosine Similarity} = \frac{\vec{u} \cdot \vec{v}}{\|\vec{u}\| \|\vec{v}\|}
  $$
- 예:
  - "king"과 "queen"의 코사인 유사도는 높음 (성별을 제외하면 유사).

---

## 워드 임베딩 학습 방법

### Word2Vec

- 단어와 주변 단어의 관계를 학습.

1. **Skip-Gram 모델**:
   - 중심 단어에서 주변 단어를 예측.
   - 예: `"orange"` → `["juice", "fruit", "color"]`
2. **Negative Sampling**:
   - 자주 등장하지 않는 단어를 샘플링하여 효율적으로 학습.

### GloVe (Global Vectors for Word Representation)

- 단어 쌍 간의 공동 발생 행렬을 기반으로 벡터를 학습.
  - 목표: 두 단어의 공통 등장 확률을 벡터 공간에서 보존.
  - 예: `"king"`과 `"queen"`은 왕족이라는 공통 문맥을 공유.

---

## 감정 분류

### 문제 정의

- 텍스트에서 감정을 추출하여 긍정, 부정, 중립으로 분류.
  - 예: `"The dessert is excellent!"` → `긍정`

### RNN 기반 감정 분류 모델

1. **입력 처리**:
   - 단어를 임베딩 벡터로 변환.
   - 예: `"The dessert is excellent"` → `[v_1, v_2, v_3, v_4]`
2. **은닉 상태 학습**:
   - 각 단어의 감정적 중요도를 학습.
3. **출력**:
   - 마지막 은닉 상태에서 softmax를 사용해 감정 분류.

---

## 워드 임베딩에서의 편향과 제거

### 문제 정의

- 워드 임베딩은 학습 데이터의 편향을 반영.
  - 예: `"Man:Computer_Programmer as Woman:Homemaker"`

### 편향 제거 단계

1. **편향 방향 식별**:
   - 편향된 단어 쌍을 분석해 주요 방향을 파악.
2. **중립화**:
   - 정의적이지 않은 단어를 편향 방향에서 제거.
3. **균형화**:
   - 편향된 쌍의 관계를 동일하게 조정.

### 활용

- 성별, 인종, 문화적 편향을 줄인 AI 시스템 개발.
- 공정성을 요구하는 응용 분야(예: 채용 시스템, 추천 시스템)에서 필수.

---
