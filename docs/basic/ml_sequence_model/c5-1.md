# 시퀀스 모델과 RNN: 포괄적인 가이드

## 소개

**시퀀스 모델(Sequence Model)**은 텍스트, 음성, 비디오 등 **순차적 데이터**를 처리하는 데 최적화된 모델입니다.
특히, **Recurrent Neural Networks (RNN)**는 이전 시간 단계의 정보를 현재 단계의 출력에 반영하여 시간적인 의존성을 학습할 수 있어 시퀀스 데이터 처리에 중요한 역할을 합니다.

RNN은 자연어 처리(NLP), 음성 인식, 시계열 데이터 분석 등에서 폭넓게 활용됩니다. 하지만 RNN에도 한계가 존재하며, 이를 보완하기 위해 여러 변형 구조가 개발되었습니다.

---

## 목차

1. [왜 시퀀스 모델인가?](#왜-시퀀스-모델인가)
2. [단어 표현](#단어-표현)
3. [순환 신경망 (RNN)](#순환-신경망-rnn)
   - [순전파](#순전파)
   - [단계별 예제](#단계별-예제)
   - [시간 역전파 (BPTT)](#시간-역전파-bptt)
4. [RNN의 한계](#rnn의-한계)
5. [RNN의 변형](#rnn의-변형)
   - [GRU (Gated Recurrent Unit)](#gru-gated-recurrent-unit)
   - [LSTM (Long Short-Term Memory)](#lstm-long-short-term-memory)
6. [응용 사례](#응용-사례)
7. [양방향 RNN](#양방향-rnn)
8. [심층 RNN](#심층-rnn)
9. [언어 모델링과 시퀀스 생성](#언어-모델링과-시퀀스-생성)

---

## 왜 시퀀스 모델인가?

### 정의

시퀀스 모델은 순차적인 입력 데이터를 받아, 그에 대응하는 순차적인 출력을 생성합니다.예를 들어:

- 음성 데이터를 텍스트로 변환하거나
- 언어 번역 작업을 수행합니다.

### 시퀀스 데이터의 특징

1. **시간적 의존성(Time Dependency)**: 데이터의 현재 값이 이전 값에 의존합니다. 예: 주식 시장 데이터.
2. **가변적인 입력/출력 길이**: 시퀀스 데이터는 정해진 길이가 없고, 상황에 따라 달라질 수 있습니다.
3. **문맥 정보**: 입력 데이터의 의미는 그 주변 데이터에 의해 결정됩니다. 예: 자연어 처리.

### 시퀀스 데이터의 예

1. **음성 인식**:
   - 입력: 오디오 신호
   - 출력: 텍스트 (예: `"The quick brown fox jumps over the lazy dog."`)
2. **감정 분석**:
   - 입력: 텍스트 리뷰 (예: `"I love this movie!"`)
   - 출력: 감정 클래스 (예: `"긍정"`)
3. **기계 번역**:
   - 입력: `"Comment ça va?"`
   - 출력: `"How are you?"`
4. **DNA 서열 분석**:
   - 입력: AGCCCCTGTGAGGAACTAG
   - 출력: 특정 유전자 발현 여부 분석
5. **음악 생성**:
   - 입력: 초기 음표들
   - 출력: 새로운 음악 시퀀스

---

## 단어 표현

### 왜 단어를 숫자로 표현해야 하는가?

컴퓨터는 텍스트 데이터를 직접 처리할 수 없으므로, 단어를 **수치 데이터로 변환**해야 합니다.
이를 통해 텍스트 데이터를 뉴럴 네트워크가 학습할 수 있습니다.

### 단어 표현 방식

1. **원-핫 인코딩 (One-Hot Encoding)**:
   - 각 단어를 고유한 이진 벡터로 표현합니다.
   - 예:
     - "Harry" → `[1, 0, 0, 0]`
     - "Potter" → `[0, 1, 0, 0]`
   - 한계점:
     - 차원이 매우 커집니다. (단어 수가 많아질수록 벡터의 길이가 증가)
     - 단어 간 의미적 관계를 나타낼 수 없습니다. (예: "king"과 "queen"은 무관한 벡터로 나타남)
2. **워드 임베딩 (Word Embedding)**:
   - 저차원 밀집 벡터를 사용하여 단어를 표현합니다.
   - 단어 간의 의미적 관계를 벡터 공간에서 학습합니다.
   - 예:
     - "king" → `[0.23, 0.56, -0.15, 0.88]`
     - "queen" → `[0.25, 0.60, -0.20, 0.85]`
   - 워드 임베딩을 사용하면 단어 간 유사성을 벡터 공간에서 측정할 수 있습니다.

#### 예제

문장: `"Harry Potter met Hermione Granger."`

- Harry → `[0.25, -0.13, 0.45, ...]`
- Potter → `[0.28, -0.10, 0.50, ...]`

워드 임베딩은 GloVe, Word2Vec, FastText 등의 모델을 사용하여 학습할 수 있습니다.

---

## 순환 신경망 (RNN)

### 개요

RNN은 이전 시간 단계의 정보를 저장하고, 이를 기반으로 현재 시간 단계의 출력을 계산합니다. 이를 통해 시간적인 의존성이 강한 데이터를 처리할 수 있습니다.

### 구조

RNN은 은닉 상태(\( a^{(t)} \))를 통해 이전 정보를 유지하며, 이를 활용해 다음 단계의 출력을 계산합니다.

- \( x^{(t)} \): 입력 벡터
- \( a^{(t)} \): 은닉 상태 (현재까지의 정보를 저장)
- \( y^{(t)} \): 출력

RNN은 입력 시퀀스의 길이에 따라 유연하게 작동하며, \( t \)단계마다 계산이 반복됩니다.

---

### 순전파

#### 핵심 공식

1. **은닉 상태 계산**:
   $$
   a^{(t)} = 	anh(W_{aa}a^{(t-1)} + W_{ax}x^{(t)} + b_a)
   $$

   - \( W_{aa} \): 이전 은닉 상태와 현재 은닉 상태를 연결하는 가중치 행렬
   - \( W_{ax} \): 입력 벡터와 은닉 상태를 연결하는 가중치 행렬
   - \( b_a \): 바이어스
2. **출력 계산**:
   $$
   y^{(t)} = 	ext{softmax}(W_{ya}a^{(t)} + b_y)
   $$

   - \( W_{ya} \): 은닉 상태에서 출력으로의 가중치
   - \( b_y \): 출력 바이어스

---

### 단계별 예제

#### 예제 문장: `"The quick brown fox"`

**1단계**: 첫 번째 단어 `"The"`

- \( x^{(1)} \): `"The"`의 임베딩 벡터
- 은닉 상태 계산:
  $$
  a^{(1)} = 	anh(W_{ax}x^{(1)} + b_a)
  $$
- 출력 계산:
  $$
  y^{(1)} = 	ext{softmax}(W_{ya}a^{(1)} + b_y)
  $$

**2단계**: 두 번째 단어 `"quick"`

- \( x^{(2)} \): `"quick"`의 임베딩 벡터
- 은닉 상태 업데이트:
  $$
  a^{(2)} = 	anh(W_{aa}a^{(1)} + W_{ax}x^{(2)} + b_a)
  $$
- 출력 계산:
  $$
  y^{(2)} = 	ext{softmax}(W_{ya}a^{(2)} + b_y)
  $$

---

### 시간 역전파 (BPTT)

시간 역전파는 RNN의 손실 값을 시간 축을 따라 역전파하여 가중치를 업데이트하는 과정입니다.

1. **손실 함수**:

   $$
   \mathcal{L}^{(t)} = -\sum_{k} y_k^{(t)} \log(\hat{y}_k^{(t)})
   $$

   $$
   \mathcal{L} = \sum_{t} \mathcal{L}^{(t)}
   $$
2. **기울기 계산**:
   모든 시간 단계에서의 은닉 상태를 포함하여 역전파를 수행합니다.

---

## RNN의 한계

1. **기울기 소실 (Vanishing Gradients)**:
   - 역전파 과정에서 기울기가 너무 작아져 학습이 어려워지는 문제.
   - 긴 시퀀스 데이터에서 발생.
2. **기울기 폭발 (Exploding Gradients)**:
   - 기울기가 비정상적으로 커져 학습이 불안정해지는 문제.
   - 이를 방지하기 위해 **기울기 클리핑(Gradient Clipping)** 기법을 사용.

---

## RNN의 변형

### GRU (Gated Recurrent Unit)

- 메모리 관리를 통해 정보 손실을 줄입니다.
- 업데이트 게이트와 리셋 게이트를 사용하여 더 간단한 구조를 제공합니다.

---

### LSTM (Long Short-Term Memory)

- 장기 의존성 문제를 해결합니다.
- **게이트 구조**를 통해 중요한 정보를 선택적으로 저장하거나 제거합니다.

#### 주요 공식:

1. **포겟 게이트**:
   $$
   f_t = \sigma(W_fx_t + U_fa_{t-1} + b_f)
   $$
2. **셀 상태 업데이트**:
   $$
   C_t = f_t \odot C_{t-1} + i_t \odot 	ilde{C}_t
   $$

---

## 응용 사례

1. **음성 인식**: 음성을 텍스트로 변환.
2. **기계 번역**: 언어 간 번역.
3. **감정 분석**: 텍스트 감정 분류.

---

## 양방향 RNN

### 정의

양방향 RNN(BRNN)은 시퀀스를 처리할 때 **정방향(forward)**과 **역방향(backward)**의 정보를 모두 활용하는 RNN 구조입니다. 이는 단어의 문맥을 더 잘 이해할 수 있게 합니다.

- 정방향 RNN: 데이터를 처음부터 끝까지 처리.
- 역방향 RNN: 데이터를 끝에서부터 처음까지 처리.
- 두 방향의 은닉 상태를 결합하여 출력 계산.

### 작동 방식

1. **정방향 은닉 상태** (\( a^{(t)}_{	ext{forward}} \)):
   $$
   a^{(t)}_{	ext{forward}} = 	anh(W_{aa}^{	ext{forward}} a^{(t-1)} + W_{ax}^{	ext{forward}} x^{(t)} + b_a^{	ext{forward}})
   $$
2. **역방향 은닉 상태** (\( a^{(t)}_{	ext{backward}} \)):
   $$
   a^{(t)}_{	ext{backward}} = 	anh(W_{aa}^{	ext{backward}} a^{(t+1)} + W_{ax}^{	ext{backward}} x^{(t)} + b_a^{	ext{backward}})
   $$
3. 최종 출력 계산:
   $$
   y^{(t)} = 	ext{softmax}(W_{ya}[a^{(t)}_{	ext{forward}}; a^{(t)}_{	ext{backward}}] + b_y)
   $$

   여기서 \( [;] \)는 두 은닉 상태를 결합(concatenation)하는 연산입니다.

### 장점

- 문맥의 양방향 정보를 활용하여 더 나은 예측 가능.
- 특히 자연어 처리(NLP)에서 중요한 역할을 함. 예: 문장 끝의 단어가 초반 단어의 의미를 결정짓는 경우.

### 활용 사례

1. **문장 태깅**: 각 단어의 품사 예측 (예: `"He eats an apple"` → `[Pronoun, Verb, Article, Noun]`).
2. **개체명 인식(NER)**: 특정 이름, 장소, 날짜 등을 태깅 (예: `"Barack Obama was born in Hawaii"` → `["Person", "Location"]`).
3. **기계 번역**: 번역 작업에서 더 정확한 문맥 분석.

---

## 심층 RNN

### 정의

심층 RNN은 RNN의 은닉 층을 여러 계층으로 쌓은 구조입니다. 이는 복잡한 패턴을 더 잘 학습할 수 있도록 도와줍니다.

- 일반 RNN: 1개의 은닉층만 사용.
- 심층 RNN: 여러 층을 사용하여 입력에서 더 높은 수준의 표현을 학습.

### 작동 방식

1. 각 계층의 출력이 다음 계층의 입력으로 사용됩니다.
2. 은닉 상태 업데이트:

   - 1층:
     $$
     a^{(1,t)} = 	anh(W_{aa}^{(1)}a^{(1,t-1)} + W_{ax}^{(1)}x^{(t)} + b_a^{(1)})
     $$
   - 2층:
     $$
     a^{(2,t)} = 	anh(W_{aa}^{(2)}a^{(2,t-1)} + W_{ax}^{(2)}a^{(1,t)} + b_a^{(2)})
     $$
3. 최종 출력 계산:

   $$
   y^{(t)} = 	ext{softmax}(W_{ya}a^{(L,t)} + b_y)
   $$

   여기서 \( L \)은 층의 개수.

### 장점

- 깊은 계층을 통해 복잡한 데이터 패턴을 학습 가능.
- 낮은 층: 기본적 특징 학습 (예: 음성의 기본 주파수).
- 높은 층: 고차원 특징 학습 (예: 음성에서 단어 구분).

### 한계

- 깊이가 깊어질수록 **기울기 소실(Vanishing Gradient)** 문제가 심화.
- 이를 해결하기 위해 LSTM, GRU와 같은 구조와 결합하여 사용.

### 활용 사례

1. **음성 인식**: 음성의 시간적 특성과 언어적 의미 학습.
2. **영상 캡션 생성**: 이미지의 특징을 시퀀스 데이터(문장)로 변환.
3. **시계열 데이터 분석**: 주식, 날씨 예측 등.

---

## 언어 모델링과 시퀀스 생성

### 언어 모델링(Language Modeling)

#### 정의

언어 모델링은 주어진 단어 시퀀스에서 다음 단어의 등장 확률을 예측하는 작업입니다.

#### 확률 계산

1. 전체 문장의 확률:
   $$
   P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^T P(w_t \mid w_1, w_2, \ldots, w_{t-1})
   $$
2. RNN을 사용한 확률 예측:
   $$
   P(w_t \mid w_1, w_2, \ldots, w_{t-1}) = 	ext{softmax}(W_{ya}a^{(t)} + b_y)
   $$

#### 데이터셋

- 대규모 텍스트 코퍼스를 학습하여 단어 간의 관계를 학습.
- 예: Wikipedia, 뉴스 기사, 책 등.

#### 활용 사례

1. **오타 교정**: "I am goin to the stor" → "I am going to the store".
2. **자동 완성**: "I want to" 입력 시 "go to the park", "eat something" 등의 추천.

---

### 시퀀스 생성(Sequence Generation)

#### 정의

RNN을 사용해 입력 데이터를 기반으로 새로운 시퀀스를 생성합니다.
예: 기계 번역, 텍스트 생성, 음악 생성.

#### 생성 방식

1. **Teacher Forcing**: 학습 중에는 실제 데이터를 모델 입력으로 사용.
2. **샘플링**: 학습된 모델로 새 데이터를 생성:

   - 초기 단어 제공: `"The weather is"`
   - 다음 단어 예측:

   $$
   w_{t+1} = 	{argmax}_{w} \, P(w \mid w_1, w_2, \ldots, w_t)
   $$

   이를 반복해 문장을 완성.

#### 예제

- 입력: `"I love machine"`
- 출력: `"learning because it is fascinating and powerful."`

#### 샘플링 기법

1. **Greedy Search**:
   - 각 단계에서 가장 높은 확률의 단어를 선택.
2. **Beam Search**:
   - 여러 경로를 유지하며 가장 가능성 높은 시퀀스를 선택.
3. **Temperature Sampling**:
   - 확률 분포를 조절하여 더 다양한 결과를 생성.

#### 활용 사례

1. **텍스트 요약**: 긴 문서를 짧게 요약.
2. **기계 번역**: 한 언어를 다른 언어로 변환.
3. **창작 작업**: 시, 소설, 음악 생성.

---
